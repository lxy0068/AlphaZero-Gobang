# coding:utf-8
import json
import os
import time
import traceback

import torch
import torch.nn.functional as F
from torch import nn, optim, cuda
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader

from .alpha_zero_mcts import AlphaZeroMCTS
from .chess_board import ChessBoard
from .policy_value_net import PolicyValueNet
from .self_play_dataset import SelfPlayData, SelfPlayDataSet


def exception_handler(train_func):
    """Decorator to handle exceptions during training.

    Saves the model and training state when an exception occurs,
    allowing recovery from unexpected errors.
    """

    def wrapper(train_pipe_line, *args, **kwargs):
        try:
            train_func(train_pipe_line)
        except BaseException as e:
            # Ignore KeyboardInterrupt but handle other exceptions.
            if not isinstance(e, KeyboardInterrupt):
                traceback.print_exc()

            # Save model state with a timestamp for recovery.
            t = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())
            train_pipe_line.save_model(
                f'last_policy_value_net_{t}.pth', 'train_losses', 'games')

    return wrapper


class PolicyValueLoss(nn.Module):
    """Loss function for policy and value heads in the AlphaZero network."""

    def __init__(self):
        super().__init__()

    def forward(self, p_hat, pi, value, z):
        """
        Computes the combined loss for policy and value outputs.

        Parameters
        ----------
        p_hat : Tensor (N, board_len^2)
            Log probabilities vector of actions.
        pi : Tensor (N, board_len^2)
            Action probabilities vector generated by MCTS.
        value : Tensor (N, )
            Value estimate for each state.
        z : Tensor (N, )
            Final game result relative to each player's reward.

        Returns
        -------
        loss : Tensor
            Sum of value loss and policy loss.
        """
        # Mean Squared Error between the estimated value and the game result.
        value_loss = F.mse_loss(value, z)
        # Cross-entropy loss for the policy (negative log likelihood).
        policy_loss = -torch.sum(pi * p_hat, dim=1).mean()
        # Total loss is the sum of value and policy losses.
        loss = value_loss + policy_loss
        return loss


class TrainModel:
    """Handles self-play, training, and testing for the AlphaZero model."""

    def __init__(self, board_len=9, lr=0.01, n_self_plays=1500, n_mcts_iters=500,
                 n_feature_planes=4, batch_size=500, start_train_size=500, check_frequency=100,
                 n_test_games=10, c_puct=4, is_use_gpu=True, is_save_game=False, **kwargs):
        """
        Initializes the training pipeline with parameters for self-play, training, and testing.

        Parameters
        ----------
        board_len : int
            Board size.
        lr : float
            Learning rate.
        n_self_plays : int
            Number of self-play games.
        n_mcts_iters : int
            Number of Monte Carlo Tree Search iterations.
        n_feature_planes : int
            Number of feature planes.
        batch_size : int
            Size of the mini-batch.
        start_train_size : int
            Minimum dataset size before starting training.
        check_frequency : int
            Frequency of testing the model.
        n_test_games : int
            Number of test games against the best historical model.
        c_puct : float
            Exploration constant.
        is_use_gpu : bool
            Whether to use GPU.
        is_save_game : bool
            Whether to save the self-play game records.
        """
        self.c_puct = c_puct
        self.is_use_gpu = is_use_gpu
        self.batch_size = batch_size
        self.n_self_plays = n_self_plays
        self.n_test_games = n_test_games
        self.n_mcts_iters = n_mcts_iters
        self.is_save_game = is_save_game
        self.check_frequency = check_frequency
        self.start_train_size = start_train_size
        self.device = torch.device(
            'cuda:0' if is_use_gpu and cuda.is_available() else 'cpu')

        # Initialize chess board and neural network
        self.chess_board = ChessBoard(board_len, n_feature_planes)
        self.policy_value_net = self.__get_policy_value_net(board_len)
        self.mcts = AlphaZeroMCTS(
            self.policy_value_net, c_puct=c_puct, n_iters=n_mcts_iters, is_self_play=True)

        # Set up optimizer, loss function, and learning rate scheduler
        self.optimizer = optim.Adam(
            self.policy_value_net.parameters(), lr=lr, weight_decay=1e-4)
        self.criterion = PolicyValueLoss()
        self.lr_scheduler = MultiStepLR(self.optimizer, milestones=[1500, 2500], gamma=0.1)

        # Create dataset for self-play data
        self.dataset = SelfPlayDataSet(board_len)

        # Load training records
        self.train_losses = self.__load_data('log/train_losses.json')
        self.games = self.__load_data('log/games.json')

    def __self_play(self):
        """Plays a self-play game to generate training data.

        Returns
        -------
        self_play_data : SelfPlayData
            Contains the feature planes, action probabilities (π), and game results (z).
        """
        # Reset the board and network state for a new game.
        self.policy_value_net.eval()
        self.chess_board.clear_board()

        pi_list, feature_planes_list, players = [], [], []
        action_list = []

        # Execute moves until the game ends.
        while True:
            action, pi = self.mcts.get_action(self.chess_board)
            feature_planes_list.append(self.chess_board.get_feature_planes())
            players.append(self.chess_board.current_player)
            action_list.append(action)
            pi_list.append(pi)
            self.chess_board.do_action(action)

            # Check if the game is over and determine the winner.
            is_over, winner = self.chess_board.is_game_over()
            if is_over:
                # Assign rewards for each player's perspective.
                z_list = [1 if i == winner else -1 for i in players] if winner else [0] * len(players)
                break

        # Reset MCTS state for the next game.
        self.mcts.reset_root()

        # Optionally save the game records.
        if self.is_save_game:
            self.games.append(action_list)

        # Create a self-play data object.
        return SelfPlayData(pi_list=pi_list, z_list=z_list, feature_planes_list=feature_planes_list)

    @exception_handler
    def train(self):
        """Main training loop for self-play and network training."""
        for i in range(self.n_self_plays):
            print(f'🏹 Playing self-play game {i + 1}...')
            self.dataset.append(self.__self_play())

            # Train the model once enough self-play data is collected.
            if len(self.dataset) >= self.start_train_size:
                data_loader = iter(DataLoader(self.dataset, self.batch_size, shuffle=True, drop_last=False))
                print('💊 Starting training...')

                self.policy_value_net.train()
                feature_planes, pi, z = next(data_loader)
                feature_planes, pi, z = feature_planes.to(self.device), pi.to(self.device), z.to(self.device)

                # Perform multiple training iterations to update the model.
                for _ in range(5):
                    p_hat, value = self.policy_value_net(feature_planes)
                    self.optimizer.zero_grad()
                    loss = self.criterion(p_hat, pi, value.flatten(), z)
                    loss.backward()
                    self.optimizer.step()
                    self.lr_scheduler.step()

                # Record training loss for analysis.
                self.train_losses.append([i, loss.item()])
                print(f"🚩 train_loss = {loss.item():<10.5f}\n")

            # Periodically evaluate the model's performance.
            if (i + 1) % self.check_frequency == 0:
                self.__test_model()

    def __test_model(self):
        """Evaluate the model against the best historical model."""
        os.makedirs('model', exist_ok=True)
        model_path = 'model/best_policy_value_net.pth'

        # Save the current model as the best model if no model exists yet.
        if not os.path.exists(model_path):
            torch.save(self.policy_value_net, model_path)
            return

        # Load and prepare the historical best model.
        best_model = torch.load(model_path)
        best_model.eval()
        best_model.set_device(self.is_use_gpu)
        mcts = AlphaZeroMCTS(best_model, self.c_puct, self.n_mcts_iters)
        self.mcts.set_self_play(False)
        self.policy_value_net.eval()

        print('🩺 Testing the current model...')
        n_wins = 0

        # Compare the current model with the best historical model in a series of games.
        for _ in range(self.n_test_games):
            self.chess_board.clear_board()
            self.mcts.reset_root()
            mcts.reset_root()
            while True:
                # Let the current model make a move.
                is_over, winner = self.__do_mcts_action(self.mcts)
                if is_over:
                    n_wins += int(winner == ChessBoard.BLACK)
                    break
                # Let the historical best model make a move.
                is_over, winner = self.__do_mcts_action(mcts)
                if is_over:
                    break

        # Save the current model if its win rate surpasses 55%.
        win_prob = n_wins / self.n_test_games
        if win_prob > 0.55:
            torch.save(self.mcts.policy_value_net, model_path)
            print(f'🥇 Current model saved as the best model with a win rate of: {win_prob:.1%}\n')
        else:
            print(f'🎃 Keeping the historical best model unchanged, current win rate: {win_prob:.1%}\n')

        self.mcts.set_self_play(True)

    def save_model(self, model_name: str, loss_name: str, game_name: str):
        """Save the model and training data to files.

        Parameters
        ----------
        model_name : str
            Model file name without suffix.
        loss_name : str
            Loss file name without suffix.
        game_name : str
            Self-play game record file name without suffix.
        """
        os.makedirs('model', exist_ok=True)

        # Save the policy-value network.
        path = f'model/{model_name}.pth'
        self.policy_value_net.eval()
        torch.save(self.policy_value_net, path)
        print(f'🎉 Model saved to {os.path.join(os.getcwd(), path)}')

        # Save loss data and game records.
        with open(f'log/{loss_name}.json', 'w', encoding='utf-8') as f:
            json.dump(self.train_losses, f)

        if self.is_save_game:
            with open(f'log/{game_name}.json', 'w', encoding='utf-8') as f:
                json.dump(self.games, f)

    def __do_mcts_action(self, mcts):
        """Executes an action using MCTS and updates the game state.

        Parameters
        ----------
        mcts : AlphaZeroMCTS
            MCTS instance for decision making.

        Returns
        -------
        is_over : bool
            Whether the game has ended.
        winner : int or None
            The winner of the game, if any.
        """
        action = mcts.get_action(self.chess_board)
        self.chess_board.do_action(action)
        return self.chess_board.is_game_over()

    def do_mcts_action(self, mcts):
        """Executes an action using MCTS and updates the game state.

        This method is intended for use during actual gameplay.

        Parameters
        ----------
        mcts : AlphaZeroMCTS
            MCTS instance for decision making.

        Returns
        -------
        is_over : bool
            Whether the game has ended.
        winner : int or None
            The winner of the game, if any.
        action : int
            The chosen action.
        """
        action = mcts.get_action(self.chess_board)
        self.chess_board.do_action(action)
        is_over, winner = self.chess_board.is_game_over()
        return is_over, winner, action

    def __get_policy_value_net(self, board_len=9):
        """Load or initialize the policy-value network.

        Tries to load the latest model if available, otherwise initializes a new one.

        Parameters
        ----------
        board_len : int
            Size of the game board.

        Returns
        -------
        net : PolicyValueNet
            Loaded or newly created policy-value network.
        """
        os.makedirs('model', exist_ok=True)

        best_model = 'best_policy_value_net.pth'
        history_models = sorted([i for i in os.listdir('model') if i.startswith('last')])

        # Load the most recent model if available.
        model = history_models[-1] if history_models else best_model
        model = f'model/{model}'
        if os.path.exists(model):
            print(f'💎 Loading model {model} ...\n')
            net = torch.load(model).to(self.device)
            net.set_device(self.is_use_gpu)
        else:
            net = PolicyValueNet(n_feature_planes=self.chess_board.n_feature_planes,
                                 is_use_gpu=self.is_use_gpu, board_len=board_len).to(self.device)

        return net

    def __load_data(self, path: str):
        """Load historical training data.

        Parameters
        ----------
        path : str
            Path to the JSON file containing training data.

        Returns
        -------
        data : list
            List of data loaded from the file, or an empty list if the file doesn't exist.
        """
        data = []
        try:
            with open(path, encoding='utf-8') as f:
                data = json.load(f)
        except:
            # Ensure the directory exists even if the file is missing.
            os.makedirs('log', exist_ok=True)

        return data